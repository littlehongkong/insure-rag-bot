import time
from pathlib import Path
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class ChromaRAGChatbot:
    def __init__(self, db_dir: str = "chroma_db", collection_name: str = "insurance_pdfs", openai_api_key: str = None):
        self.db_dir = str(Path(__file__).resolve().parent.parent.parent / db_dir)
        self.collection_name = collection_name
        self.embeddings = None
        self.db = None
        self.qa_chain = None
        self.llm = None
        self.openai_api_key = openai_api_key

        # OpenAI API í‚¤ ì„¤ì •
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
        elif not os.getenv("OPENAI_API_KEY"):
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ì„¸ìš”.")

        print(f"[Init] OpenAI RAG Chatbot initialized")
        print(f"[Init] ChromaRAGChatbot initialized with DB dir: {self.db_dir}, collection: {self.collection_name}")

    def load_embeddings(self):
        start_time = time.time()
        self.embeddings = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print(f"[Time] Embedding ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {time.time() - start_time:.2f}ì´ˆ")

    def load_vectorstore(self):
        if not self.embeddings:
            self.load_embeddings()

        start_time = time.time()

        # âœ… ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ì´ˆê¸°í™”
        self.db = Chroma(
            persist_directory=self.db_dir,
            collection_name=self.collection_name,
            embedding_function=self.embeddings
        )

        # ë¬¸ì„œ ìˆ˜ í™•ì¸
        try:
            docs = self.db.get()
            print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(docs['documents'])}")
            if docs['documents']:
                print("ë¬¸ì„œ ìƒ˜í”Œ:", docs['documents'][:1])  # ìƒ˜í”Œ ìˆ˜ ì¤„ì„
        except Exception as e:
            print(f"ë¬¸ì„œ ì¡°íšŒ ì˜¤ë¥˜: {e}")

        print(f"[Time] Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì™„ë£Œ: {time.time() - start_time:.2f}ì´ˆ")

    def load_llm(self, model: str = "gpt-5-nano", temperature: float = 0.1):
        """âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ ì‚¬ìš© ë° íŒŒë¼ë¯¸í„° ìµœì í™”"""
        start_time = time.time()

        self.llm = ChatOpenAI(
            api_key=self.openai_api_key,
            model=model,
            max_tokens=4096  # ì‘ë‹µ ê¸¸ì´ ì œí•œ
        )

        print(f"[Time] OpenAI {model} ë¡œë”© ì™„ë£Œ: {time.time() - start_time:.2f}ì´ˆ")

    def build_chain(self):
        if not self.db:
            self.load_vectorstore()
        if not self.llm:
            self.load_llm()

        start_time = time.time()

        # âœ… ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ìµœì í™”
        retriever = self.db.as_retriever(search_kwargs={"k": 3})

        # âœ… í•œêµ­ì–´ ìµœì í™” í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        prompt_template = """ë‹¤ìŒ ë³´í—˜ ê´€ë ¨ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•œ í•œêµ­ì–´ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ (í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ):"""

        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )

        # âœ… stuff ë°©ì‹ ì‚¬ìš© (map_reduceë³´ë‹¤ ë¹ ë¥´ê³  ì•ˆì •ì )
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",  # map_reduce â†’ stuffë¡œ ë³€ê²½
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )
        print(f"[Time] QA ì²´ì¸ ë¹Œë“œ ì™„ë£Œ: {time.time() - start_time:.2f}ì´ˆ")

    def ask(self, question: str) -> str:
        """âœ… ë””ë²„ê¹… ì •ë³´ ì¶”ê°€ ë° ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”"""
        if not self.qa_chain:
            self.build_chain()

        print(f"[Query] ì§ˆë¬¸ ì…ë ¥ë¨: {question}")

        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        try:
            retrieved_docs = self.qa_chain.retriever.get_relevant_documents(question)
            print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
        except Exception as e:
            print(f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        start_time = time.time()
        try:
            result = self.qa_chain.invoke({"query": question})  # invoke ì‚¬ìš©
            print(f"[Time] ì§ˆë¬¸ ì‘ë‹µ ì²˜ë¦¬ ì™„ë£Œ: {time.time() - start_time:.2f}ì´ˆ")

            # âœ… ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
            print(f"[Debug] ê²°ê³¼ í‚¤ë“¤: {list(result.keys())}")
            print(f"[Debug] result ê¸¸ì´: {len(result.get('result', ''))}")

            if not result.get('result', '').strip():
                print("âš ï¸ OpenAI ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ask_direct() ë°©ì‹ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
                return self.ask_direct(question)

            return result["result"]

        except Exception as e:
            print(f"âŒ QA ì²´ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            print(f"âŒ ask_direct() ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            return self.ask_direct(question)

    def ask_direct(self, question: str, model: str = "gpt-5-nano") -> str:
        """âœ… ì§ì ‘ í˜¸ì¶œ ë°©ì‹ - ë” ì•ˆì •ì ì´ê³  ë¹ ë¦„"""
        if not self.db:
            self.load_vectorstore()

        print(f"\nâš¡ [Direct Mode] {question}")
        total_start = time.time()

        # 1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰
        search_start = time.time()
        docs = self.db.similarity_search(question, k=3)
        search_time = time.time() - search_start
        print(f"ğŸ“„ ê²€ìƒ‰ ì™„ë£Œ: {search_time:.3f}ì´ˆ ({len(docs)}ê°œ ë¬¸ì„œ)")

        if not docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for i, doc in enumerate(docs, 1):
            content = doc.page_content[:300].strip()
            meta = doc.metadata

            # ìƒí’ˆëª…, ì½”ë“œ ë“± í•„ìš”í•œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì²˜ë¦¬)
            product_name = meta.get("tp_name", "ì•Œ ìˆ˜ ì—†ìŒ ìƒí’ˆ")
            product_code = meta.get("tp_code", "ì½”ë“œ ì—†ìŒ")

            if content:
                context_parts.append(
                    f"[ë¬¸ì„œ{i} | ìƒí’ˆëª…: {product_name} | ì½”ë“œ: {product_code}]\n{content}"
                )

        context = "\n\n".join(context_parts)
        print(f"ğŸ“ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)}ì")

        # 3ë‹¨ê³„: OpenAI API ì§ì ‘ í˜¸ì¶œ
        try:
            if not self.llm or self.llm.model_name != model:
                self.load_llm(model=model)

            prompt = f"""
            ë‹¤ìŒì€ ì„œë¡œ ë‹¤ë¥¸ ë³´í—˜ ìƒí’ˆì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.
            ê° ë¬¸ì„œì˜ [ìƒí’ˆëª…] ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ìƒí’ˆì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

            {context}

            ì‚¬ìš©ì ì§ˆë¬¸: {question}

            ë³´í—˜ ì „ë¬¸ê°€ì˜ ë‹µë³€:"""

            api_start = time.time()
            response = self.llm.invoke(prompt)
            api_time = time.time() - api_start
            total_time = time.time() - total_start

            answer = response.content.strip()

            print(f"ğŸ¤– OpenAI API: {api_time:.3f}ì´ˆ")
            print(f"ğŸ¯ ì „ì²´ ì‹œê°„: {total_time:.3f}ì´ˆ")
            print(f"ğŸ“ ë‹µë³€ ê¸¸ì´: {len(answer)}ì")

            if not answer:
                return "OpenAIë¡œë¶€í„° ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            return answer

        except Exception as e:
            print(f"âŒ Direct API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    def ask_gpt4(self, question: str) -> str:
        """GPT-4ë¥¼ ì‚¬ìš©í•œ ê³ í’ˆì§ˆ ì‘ë‹µ"""
        return self.ask_direct(question, model="gpt-4")

    def test_connection(self):
        """âœ… OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            if not self.llm:
                self.load_llm()

            test_response = self.llm.invoke("ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
            print(f"âœ… OpenAI ì—°ê²° ì„±ê³µ: {test_response.content[:50]}...")
            return True
        except Exception as e:
            print(f"âŒ OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
            return False


# ì˜ˆì‹œ ì‹¤í–‰
if __name__ == "__main__":

    openai_api_key = os.getenv("OPENAI_API_KEY")

    if not openai_api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        exit(1)

    chatbot = ChromaRAGChatbot(
        db_dir="/Users/benjamin/PycharmProjects/insure-rag-bot/chroma_db",
        collection_name="insurance_pdfs",
        openai_api_key=openai_api_key
    )

    # âœ… ì—°ê²° í…ŒìŠ¤íŠ¸
    print("\nğŸ”§ OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
    if not chatbot.test_connection():
        print("âŒ OpenAI ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit(1)

    # âœ… ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 60)
    print("ğŸš€ RAG ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    question = "ì–´ë¦°ì´ ì‹¤ì†ë³´í—˜ì˜ ë³´ì¥ ë²”ìœ„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"

    # print("\n1ï¸âƒ£ ê¸°ë³¸ ë°©ì‹ (RetrievalQA):")
    # response1 = chatbot.ask(question)
    # print(f"\n[Answer] {response1}")

    print(f"\n2ï¸âƒ£ ì§ì ‘ ë°©ì‹ (ë” ë¹ ë¦„):")
    response2 = chatbot.ask_direct(question)
    print(f"\n[Direct Answer] {response2}")

    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")