import time
import logging
from pathlib import Path
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
import os
from dotenv import load_dotenv
from typing import List, Tuple, Optional, Dict, Any
from contextlib import contextmanager

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ChromaRAGChatbot:
    def __init__(self,
                 db_dir: str = "chroma_db",
                 collection_name: str = "insurance_pdfs",
                 openai_api_key: Optional[str] = None,
                 default_model: str = "gpt-5-nano",  # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ë¡œ ë³€ê²½
                 temperature: float = 0.1):

        self.db_dir = str(Path(__file__).resolve().parent.parent.parent / db_dir)
        self.collection_name = collection_name
        self.default_model = default_model
        self.temperature = temperature

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.embeddings = None
        self.db = None
        self.qa_chain = None
        self.conversational_chain = None
        self.memory = None
        self.llm = None

        # OpenAI API í‚¤ ì„¤ì •
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ì„¸ìš”.")

        os.environ["OPENAI_API_KEY"] = self.openai_api_key
        logger.info(f"ChromaRAG ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ - DB: {self.db_dir}, Collection: {self.collection_name}")

    @contextmanager
    def _timer(self, operation: str):
        """ì‹œê°„ ì¸¡ì • ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        start_time = time.time()
        try:
            yield
        finally:
            elapsed = time.time() - start_time
            logger.info(f"[Timer] {operation}: {elapsed:.3f}ì´ˆ")

    def load_embeddings(self) -> None:
        """ì„ë² ë”© ëª¨ë¸ ë¡œë”©"""
        if self.embeddings is not None:
            return

        with self._timer("ì„ë² ë”© ëª¨ë¸ ë¡œë”©"):
            self.embeddings = HuggingFaceEmbeddings(
                model_name="jhgan/ko-sroberta-multitask",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )

    def load_vectorstore(self) -> None:
        """ë²¡í„° ìŠ¤í† ì–´ ë¡œë”©"""
        if self.db is not None:
            return

        if not self.embeddings:
            self.load_embeddings()

        with self._timer("ë²¡í„°ìŠ¤í† ì–´ ë¡œë”©"):
            try:
                self.db = Chroma(
                    persist_directory=self.db_dir,
                    collection_name=self.collection_name,
                    embedding_function=self.embeddings
                )

                # ë¬¸ì„œ ìˆ˜ í™•ì¸
                docs = self.db.get()
                doc_count = len(docs.get('documents', []))
                logger.info(f"ë¡œë”©ëœ ë¬¸ì„œ ìˆ˜: {doc_count}")

                if doc_count == 0:
                    logger.warning("ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")

            except Exception as e:
                logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise

    def load_llm(self, model: Optional[str] = None, temperature: Optional[float] = None) -> None:
        """LLM ëª¨ë¸ ë¡œë”©"""
        model = model or self.default_model
        temperature = temperature if temperature is not None else self.temperature

        # ì´ë¯¸ ê°™ì€ ëª¨ë¸ì´ ë¡œë”©ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if self.llm and hasattr(self.llm, 'model_name') and self.llm.model_name == model:
            return

        with self._timer(f"OpenAI {model} ë¡œë”©"):
            try:
                self.llm = ChatOpenAI(
                    api_key=self.openai_api_key,
                    model=model,
                    temperature=temperature
                )
            except Exception as e:
                logger.error(f"LLM ë¡œë”© ì‹¤íŒ¨: {e}")
                raise

    def _get_korean_prompt_template(self) -> str:
        """í•œêµ­ì–´ ìµœì í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        return """ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë³´í—˜ ê´€ë ¨ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•œ í•œêµ­ì–´ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì§€ì¹¨:
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì˜ ì •ë³´ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ í‘œí˜„í•˜ì„¸ìš”
3. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ë³´í—˜ ì§€ì‹ìœ¼ë¡œ ëŒ€ë‹µí•˜ì§€ ë§ˆì„¸ìš”
4. ë‹µë³€ì€ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
5. ë³´í—˜ ìƒí’ˆëª…, ì½”ë“œ, ì¡°ê±´ ë“±ì€ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”

ë‹µë³€:"""

    def build_chain(self) -> None:
        """QA ì²´ì¸ êµ¬ì¶•"""
        if self.qa_chain is not None:
            return

        if not self.db:
            self.load_vectorstore()
        if not self.llm:
            self.load_llm()

        with self._timer("QA ì²´ì¸ êµ¬ì¶•"):
            try:
                # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ìµœì í™” - kê°’ ì¦ê°€ë¡œ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
                retriever = self.db.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 5}  # 3 â†’ 5ë¡œ ì¦ê°€
                )

                prompt = PromptTemplate(
                    template=self._get_korean_prompt_template(),
                    input_variables=["context", "question"]
                )

                self.qa_chain = RetrievalQA.from_chain_type(
                    llm=self.llm,
                    chain_type="stuff",
                    retriever=retriever,
                    return_source_documents=True,
                    chain_type_kwargs={"prompt": prompt}
                )
            except Exception as e:
                logger.error(f"QA ì²´ì¸ êµ¬ì¶• ì‹¤íŒ¨: {e}")
                raise

    def build_conversational_chain(self, memory_window: int = 5) -> None:
        """ëŒ€í™”í˜• ì²´ì¸ êµ¬ì¶•"""
        if self.conversational_chain is not None:
            return

        if not self.db:
            self.load_vectorstore()
        if not self.llm:
            self.load_llm()

        with self._timer("ëŒ€í™”í˜• ì²´ì¸ êµ¬ì¶•"):
            try:
                # ë©”ëª¨ë¦¬ ì„¤ì •
                self.memory = ConversationBufferWindowMemory(
                    k=memory_window,
                    memory_key="chat_history",
                    return_messages=True,
                    output_key='answer'
                )

                # ê²€ìƒ‰ê¸° ì„¤ì • - ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
                retriever = self.db.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 5}
                )

                self.conversational_chain = ConversationalRetrievalChain.from_llm(
                    llm=self.llm,
                    retriever=retriever,
                    memory=self.memory,
                    return_source_documents=True,
                    verbose=False  # ë¡œê·¸ ì •ë¦¬ë¥¼ ìœ„í•´ Falseë¡œ ë³€ê²½
                )
            except Exception as e:
                logger.error(f"ëŒ€í™”í˜• ì²´ì¸ êµ¬ì¶• ì‹¤íŒ¨: {e}")
                raise

    def _extract_metadata_info(self, metadata: Dict[str, Any]) -> Dict[str, str]:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        return {
            "product_name": metadata.get("item_name", metadata.get("tp_name", "ì•Œ ìˆ˜ ì—†ëŠ” ìƒí’ˆ")),
            "product_code": metadata.get("item_code", metadata.get("tp_code", "ì½”ë“œ ì—†ìŒ")),
            "insurer_name": metadata.get("insurer_name", "ë³´í—˜ì‚¬ ì •ë³´ ì—†ìŒ"),
            "page": str(metadata.get("page", 0) + 1),  # 0-based â†’ 1-based
            "is_table": "í‘œ ë°ì´í„°" if metadata.get("is_table", False) else "ì¼ë°˜ í…ìŠ¤íŠ¸"
        }

    def ask_with_detailed_context(self, question: str, k: int = 5, model: Optional[str] = None) -> Dict[str, Any]:
        """ìƒì„¸í•œ ì»¨í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ì§ˆë¬¸ ì²˜ë¦¬"""
        if not self.db:
            self.load_vectorstore()

        logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {question}")

        with self._timer("ì „ì²´ ì²˜ë¦¬"):
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            with self._timer("ë¬¸ì„œ ê²€ìƒ‰"):
                docs = self.db.similarity_search_with_score(question, k=k)

            if not docs:
                return {
                    "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "context_used": ""
                }

            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ë” ìƒì„¸í•œ ì •ë³´ í¬í•¨
            context_parts = []
            source_info = []

            for i, (doc, score) in enumerate(docs, 1):
                content = doc.page_content.strip()
                if not content:
                    continue

                meta_info = self._extract_metadata_info(doc.metadata)

                # ì»¨í…ìŠ¤íŠ¸ì— ë” ìì„¸í•œ ë©”íƒ€ì •ë³´ í¬í•¨
                context_part = f"""[ë¬¸ì„œ {i}] 
ìƒí’ˆëª…: {meta_info['product_name']}
ìƒí’ˆì½”ë“œ: {meta_info['product_code']}  
ë³´í—˜ì‚¬: {meta_info['insurer_name']}
í˜ì´ì§€: {meta_info['page']}
ìœ í˜•: {meta_info['is_table']}
ìœ ì‚¬ë„: {score:.3f}

ë‚´ìš©:
{content[:500]}"""  # 300ì â†’ 500ìë¡œ ì¦ê°€

                context_parts.append(context_part)

                # ì†ŒìŠ¤ ì •ë³´ ì €ì¥
                source_info.append({
                    "product_name": meta_info['product_name'],
                    "product_code": meta_info['product_code'],
                    "page": meta_info['page'],
                    "similarity_score": score,
                    "content_preview": content[:100]
                })

            context = "\n\n" + "=" * 50 + "\n\n".join(context_parts)

            # 3. LLM í˜¸ì¶œ
            if not self.llm or (model and hasattr(self.llm, 'model_name') and self.llm.model_name != model):
                self.load_llm(model=model)

            prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ê° ë¬¸ì„œëŠ” [ë¬¸ì„œ ë²ˆí˜¸], ìƒí’ˆëª…, ìƒí’ˆì½”ë“œ, ë³´í—˜ì‚¬, í˜ì´ì§€, ìœ í˜•, ìœ ì‚¬ë„ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹µë³€ ì‘ì„± ì§€ì¹¨:
1. ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œ(ìœ ì‚¬ë„ê°€ ë†’ì€)ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ì¡°í•˜ì„¸ìš”
2. ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ë³´ì™„ì ì¸ ì •ë³´ê°€ ìˆë‹¤ë©´ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”  
3. ìƒí’ˆëª…ê³¼ ì½”ë“œë¥¼ ì •í™•íˆ ëª…ì‹œí•˜ì„¸ìš”
4. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
5. ë‹µë³€ êµ¬ì¡°: ìš”ì•½ â†’ ìƒì„¸ ì„¤ëª… â†’ ì°¸ê³  ë¬¸ì„œ ì •ë³´
6. ì§ˆë¬¸ì˜ ì˜ë„ì— ë§ì¶° êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì¡°ê±´ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.

ì „ë¬¸ê°€ ë‹µë³€:"""

            with self._timer("OpenAI API í˜¸ì¶œ"):
                try:
                    response = self.llm.invoke(prompt)
                    answer = response.content.strip()
                except Exception as e:
                    logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    return {
                        "answer": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                        "sources": source_info,
                        "context_used": context
                    }

            logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ - ê¸¸ì´: {len(answer)}ì, ì°¸ì¡° ë¬¸ì„œ: {len(source_info)}ê°œ")

            return {
                "answer": answer,
                "sources": source_info,
                "context_used": context
            }

    def ask_conversational(self, question: str, memory_window: int = 5) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ëŠ” ì§ˆë¬¸ ë°©ì‹"""
        if not self.conversational_chain:
            self.build_conversational_chain(memory_window)

        logger.info(f"ëŒ€í™”í˜• ì§ˆë¬¸: {question}")

        with self._timer("ëŒ€í™”í˜• ì²˜ë¦¬"):
            try:
                result = self.conversational_chain.invoke({"question": question})
                answer = result.get("answer", "").strip()

                if not answer:
                    logger.warning("ëŒ€í™”í˜• ì‘ë‹µì´ ë¹„ì–´ìˆìŒ - ìƒì„¸ ëª¨ë“œë¡œ ì¬ì‹œë„")
                    detailed_result = self.ask_with_detailed_context(question)
                    return detailed_result["answer"]

                return answer

            except Exception as e:
                logger.error(f"ëŒ€í™”í˜• ì²´ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                # í´ë°±ìœ¼ë¡œ ìƒì„¸ ëª¨ë“œ ì‚¬ìš©
                detailed_result = self.ask_with_detailed_context(question)
                return detailed_result["answer"]

    def ask(self, question: str) -> str:
        """ê¸°ë³¸ ì§ˆë¬¸ ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        result = self.ask_with_detailed_context(question)
        return result["answer"]

    def clear_memory(self) -> None:
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        if self.memory:
            self.memory.clear()
            logger.info("ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")

    def get_memory_messages(self) -> List:
        """í˜„ì¬ ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ë©”ì‹œì§€ ë°˜í™˜"""
        if self.memory and hasattr(self.memory, 'chat_memory'):
            return self.memory.chat_memory.messages
        return []

    def test_connection(self) -> bool:
        """OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            if not self.llm:
                self.load_llm()

            test_response = self.llm.invoke("ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
            logger.info(f"OpenAI ì—°ê²° ì„±ê³µ: {test_response.content[:50]}...")
            return True
        except Exception as e:
            logger.error(f"OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def get_collection_info(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        if not self.db:
            self.load_vectorstore()

        try:
            collection_data = self.db.get()
            doc_count = len(collection_data.get('documents', []))

            # ìƒ˜í”Œ ë©”íƒ€ë°ì´í„° ë¶„ì„
            sample_meta = {}
            if collection_data.get('metadatas'):
                sample_meta = collection_data['metadatas'][0] if collection_data['metadatas'] else {}

            return {
                "document_count": doc_count,
                "collection_name": self.collection_name,
                "sample_metadata_keys": list(sample_meta.keys()) if sample_meta else [],
                "db_path": self.db_dir
            }
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # API í‚¤ í™•ì¸
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        exit(1)

    # ì±—ë´‡ ì´ˆê¸°í™”
    chatbot = ChromaRAGChatbot(
        db_dir="chroma_db",  # ê²½ë¡œ ìˆ˜ì • í•„ìš”ì‹œ ë³€ê²½
        collection_name="insurance_pdfs",
        openai_api_key=openai_api_key,
        default_model="gpt-5-nano"  # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ ì‚¬ìš©
    )

    # ì—°ê²° í…ŒìŠ¤íŠ¸
    print("\nğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì¤‘...")
    if not chatbot.test_connection():
        print("âŒ OpenAI ì—°ê²° ì‹¤íŒ¨")
        exit(1)

    # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
    collection_info = chatbot.get_collection_info()
    print(f"ğŸ“Š ì»¬ë ‰ì…˜ ì •ë³´: {collection_info}")

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    print("\n" + "=" * 60)
    print("ğŸš€ ê°œì„ ëœ RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    test_questions = [
        "ë¬´ë°°ë‹¹THEì±„ìš°ëŠ”335ê°„í¸ê³ ì§€ì¢…ì‹ ë³´í—˜(í•´ì•½í™˜ê¸‰ê¸ˆì¼ë¶€ì§€ê¸‰í˜•)_ì²´ì¦í˜• ë³´í—˜ì˜ íŠ¹ì§•ì„ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì´ ë³´í—˜ì˜ ë³´ì¥ ë‚´ìš©ê³¼ ê°€ì… ì¡°ê±´ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "í•´ì•½í™˜ê¸‰ê¸ˆ ì¼ë¶€ì§€ê¸‰í˜•ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    ]

    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ ì§ˆë¬¸ {i}: {question}")
        print("-" * 50)

        # ìƒì„¸ ì»¨í…ìŠ¤íŠ¸ë¡œ ì§ˆë¬¸
        result = chatbot.ask_with_detailed_context(question)

        print(f"ğŸ’¡ ë‹µë³€:\n{result['answer']}")
        print(f"\nğŸ“š ì°¸ì¡° ë¬¸ì„œ ìˆ˜: {len(result['sources'])}")

        # ì°¸ì¡° ë¬¸ì„œ ì •ë³´ ì¶œë ¥
        for j, source in enumerate(result['sources'][:3], 1):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
            print(f"   {j}. {source['product_name']} (ì½”ë“œ: {source['product_code']}) "
                  f"- ìœ ì‚¬ë„: {source['similarity_score']:.3f}")

    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")