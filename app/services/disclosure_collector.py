import requests
import supabase
import os
from datetime import datetime, timedelta
import logging
from typing import List, Dict, Optional
from dotenv import load_dotenv
import time
import re
import json
import glob

load_dotenv()  # .env íŒŒì¼ ë¡œë“œ

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


class DisclosureCollector:
    def __init__(self, raw_data_dir: str = "raw_data", processed_data_dir: str = "processed_data"):
        self.base_url = "https://kpub.knia.or.kr"
        self.supabase_url = os.getenv('SUPABASE_URL')
        self.supabase_key = os.getenv('SUPABASE_KEY')
        self.supabase = supabase.create_client(self.supabase_url, self.supabase_key)

        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.raw_data_dir = raw_data_dir
        self.processed_data_dir = processed_data_dir
        self._ensure_directories()
        self.table_info = {
            'PB': {
                'table_name': 'insurance_products_raw',
                'on_conflict': "TP_CODE,TP_PAY_NAME,popup_code,ROW_NUM"
            },
            'PC': {
                'table_name': 'insurance_products_long_term_savings_raw',
                'on_conflict': "TP_CODE,TP_PAY_NAME,popup_code,PERIOD,NUM1"
            }
        }

        # ì˜¤ëŠ˜ ë‚ ì§œ
        self.today = datetime.now().strftime('%Y-%m-%d')
        self.date_str = datetime.now().strftime('%Y%m%d')

        self.popup = {
            1: {
                'menu_name': 'ì¥ê¸°ë³´ì¥ì„±',
                'data': {
                    "í™”ì¬": "PB11",
                    "ì¢…í•©": "PB12",
                    "ìš´ì „ì": "PB13",
                    "ì–´ë¦°ì´": "PB24",
                    "ì¹˜ì•„": "PB25",
                    "ê°„ë³‘Â·ì¹˜ë§¤": "PB16",
                    "ì•”": "PB22",
                    "ìƒí•´": "PB14",
                    "ì§ˆë³‘": "PB15",
                    "ë¹„ìš©": "PB18",
                    "ê¸°íƒ€": "PB17"
                }
            },
            2: {
                'menu_name': 'ì¥ê¸°ì €ì¶•ì„±',
                'data': {
                    "ì¥ê¸°í™”ì¬ ë° ì¢…í•©ë³´í—˜": "PC11",
                    "ì¥ê¸°ìƒí•´ë³´í—˜ ë° ê¸°íƒ€": "PC12",
                }
            },
            3: {
                'menu_name': 'ì‹¤ì†ì˜ë£Œë³´í—˜',
                'data': {
                    "ì‹¤ì†ì˜ë£Œë³´í—˜(4ì„¸ëŒ€)": "PB26",
                    "ë…¸í›„ì‹¤ì†ì˜ë£Œë³´í—˜": "PB21",
                    "ìœ ë³‘ë ¥ìì‹¤ì†ë³´í—˜": "PB23",
                }
            },
            4: {
                'menu_name': 'ì—°ê¸ˆì €ì¶•',
                'data': {
                    "ì—°ê¸ˆì €ì¶•": "PC99"
                }
            }
        }

    def _ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        os.makedirs(self.raw_data_dir, exist_ok=True)
        os.makedirs(self.processed_data_dir, exist_ok=True)
        logging.info(f"âœ… ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ: {self.raw_data_dir}, {self.processed_data_dir}")

    def save_raw_response(self, response_text: str, popup_code: str, category_main: str, category_sub: str) -> str:
        """ì›ì²œ API ì‘ë‹µì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # íŒŒì¼ëª…ì— ì¹´í…Œê³ ë¦¬ ì •ë³´ í¬í•¨
            safe_category_sub = re.sub(r'[^\w\-_]', '_', category_sub)  # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
            filename = os.path.join(
                self.raw_data_dir,
                f"{self.date_str}_{popup_code}_{safe_category_sub}_raw.json"
            )

            # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥
            raw_data = {
                "timestamp": datetime.utcnow().isoformat() + 'Z',
                "popup_code": popup_code,
                "category_main": category_main,
                "category_sub": category_sub,
                "raw_response": response_text
            }

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(raw_data, f, ensure_ascii=False, indent=2)

            logging.info(f"âœ… ì›ì²œ ë°ì´í„° ì €ì¥: {filename}")
            return filename

        except Exception as e:
            logging.error(f"âŒ ì›ì²œ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def collect_raw_data(self) -> bool:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ì›ì²œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
        url = f"{self.base_url}/popup/disclosureList.do"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'X-Requested-With': 'XMLHttpRequest',
            'Origin': 'https://kpub.knia.or.kr',
            'Referer': 'https://kpub.knia.or.kr/productDisc/longTermGuarantee/fireInsurance.do'
        }

        total_categories = sum(len(self.popup[key]['data']) for key in self.popup.keys())
        current_count = 0
        success_count = 0

        logging.info(f"ğŸš€ ì›ì²œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ - ì´ {total_categories}ê°œ ì¹´í…Œê³ ë¦¬")

        for tab_key in self.popup.keys():
            category_main = self.popup[tab_key]['menu_name']

            for category_sub, popup_code in self.popup[tab_key]['data'].items():
                current_count += 1
                logging.info(f"ğŸ“¥ ìˆ˜ì§‘ ì¤‘ ({current_count}/{total_categories}): {category_main} > {category_sub}")

                payload = {
                    'tabType': str(tab_key),
                    'tptyCode': popup_code,
                    'refreshYn': '',
                    'detailYn': '',
                    'pCode': '',
                    'channel': '',
                    'prdNm': '',
                    'payNm': '',
                    'payReason': '',
                    '__sort__': '1 asc'
                }

                try:
                    # SSL ê²€ì¦ê³¼ í•¨ê»˜ ìš”ì²­ ì‹œë„
                    try:
                        response = requests.post(url, data=payload, headers=headers, verify=True, timeout=30)
                        response.raise_for_status()
                    except requests.exceptions.SSLError as ssl_err:
                        logging.warning(f"SSL ê²€ì¦ ì‹¤íŒ¨, SSL ì—†ì´ ì¬ì‹œë„: {ssl_err}")
                        import urllib3
                        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
                        response = requests.post(url, data=payload, headers=headers, verify=False, timeout=30)
                        response.raise_for_status()

                    # ì›ì²œ ë°ì´í„° ì €ì¥
                    saved_file = self.save_raw_response(
                        response.text, popup_code, category_main, category_sub
                    )

                    if saved_file:
                        success_count += 1
                        logging.info(f"âœ… ì €ì¥ ì™„ë£Œ ({success_count}/{current_count})")
                    else:
                        logging.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {category_sub}")

                    # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
                    time.sleep(1)

                except requests.exceptions.RequestException as e:
                    logging.error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨ ({category_sub}): {e}")
                except Exception as e:
                    logging.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ({category_sub}): {e}")

        logging.info(f"ğŸ‰ ì›ì²œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{total_categories}ê°œ ì„±ê³µ")
        return success_count > 0

    def parse_api_response(self, api_response_string: str) -> dict:
        """
        API ì‘ë‹µì„ íŒŒì‹±í•˜ê³  ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
        ë¶ˆì™„ì „í•˜ê±°ë‚˜ ì˜ëª»ëœ JSON í˜•ì‹ì„ ë³´ì •í•˜ì—¬ íŒŒì‹±ì„ ì‹œë„í•©ë‹ˆë‹¤.
        """
        # 1. ë¬¸ìì—´ ì•ë’¤ ê³µë°± ì œê±°
        cleaned_string = api_response_string.strip()

        # 2. HTML íƒœê·¸ (<i class="noViewItem">...</i> íŒ¨í„´) ì œê±°
        cleaned_string = re.sub(r'<i[^>]*?>.*?</i>', '', cleaned_string, flags=re.DOTALL)

        # 3. ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ '\r', '\n', '\t'ì„ JSON í‘œì¤€ì— ë§ê²Œ '\\r', '\\n', '\\t'ìœ¼ë¡œ ë³€ê²½
        #    ì´ ë‹¨ê³„ëŠ” ìœ ì§€í•©ë‹ˆë‹¤.
        corrected_string = cleaned_string.replace('\r', '\\r').replace('\n', '\\n').replace('\t', '\\t')

        # 4. í‚¤ì— ë”°ì˜´í‘œê°€ ì—†ëŠ” ê²½ìš° í°ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°
        corrected_string = re.sub(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', corrected_string)

        # 5. ì‘ì€ë”°ì˜´í‘œë¡œ ë¬¶ì¸ ë¬¸ìì—´ ê°’ì„ í°ë”°ì˜´í‘œë¡œ ë³€ê²½
        corrected_string = re.sub(r"'([^']*)'", r'"\1"', corrected_string)

        # âœ¨ 6. 'TP_ETC' í•„ë“œì˜ ê°’ì—ì„œ ë°œìƒí•˜ëŠ” ì˜ëª»ëœ ë”°ì˜´í‘œ ë° ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ìì—´ ì²˜ë¦¬
        #    "TP_ETC":""ê°’"" í˜•íƒœ ì²˜ë¦¬ (ì´ì „ ë‹¨ê³„ì—ì„œ ì¶”ê°€ëœ ë¡œì§)
        corrected_string = re.sub(r'"TP_ETC":\s*""(.*?)"",', r'"TP_ETC":"\1",', corrected_string)

        try:
            # 1ì°¨ íŒŒì‹± ì‹œë„ (ê°€ì¥ ê¹”ë”í•œ ìƒíƒœ)
            data = json.loads(corrected_string)
            return data
        except json.JSONDecodeError as e:
            logging.warning(f"1ì°¨ ì •ë¦¬ í›„ JSON íŒŒì‹± ì˜¤ë¥˜: {e}. ì¶”ê°€ ë³´ì • ì‹œë„.")
            # 2ì°¨ ë³´ì •: ì‘ì€ ë”°ì˜´í‘œë¥¼ í° ë”°ì˜´í‘œë¡œ ë³€ê²½ (í‚¤ì— ëŒ€í•œ ì²˜ë¦¬ëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì‹œë„ë¨)
            temp_string = cleaned_string.replace("'", '"')
            try:
                data = json.loads(temp_string)
                return data
            except json.JSONDecodeError as inner_e:
                logging.warning(f"2ì°¨ ì •ë¦¬ í›„ì—ë„ íŒŒì‹± ì‹¤íŒ¨: {inner_e}")

        # ë§ˆì§€ë§‰ ì‹œë„: Regexë¥¼ í†µí•œ ìµœì¢… ë³´ì •
        try:
            # JSON ë°°ì—´ì˜ ì‹œì‘ê³¼ ëì´ ë¶ˆë¶„ëª…í•  ë•Œ ì „ì²´ë¥¼ [ ]ë¡œ ê°ì‹¸ê¸°
            if not cleaned_string.startswith('[') and not cleaned_string.endswith(']'):
                # ë§Œì•½ "list": [ ... ] í˜•íƒœì˜ ìµœìƒìœ„ ê°ì²´ê°€ ì•„ë‹ˆë¼,
                # ë°”ë¡œ ë°°ì—´ ë‚´ìš©ë§Œ ë„˜ì–´ì˜¤ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„ (ë“œë¬¼ì§€ë§Œ ë°œìƒ ê°€ëŠ¥)
                if cleaned_string.startswith('{') and cleaned_string.endswith('}'):
                    # ë‹¨ì¼ ê°ì²´ì¼ ê²½ìš° ë°°ì—´ë¡œ ê°ì‹¸ê¸° (ex: {"a":1} -> [{"a":1}])
                    temp_string = f"[{cleaned_string}]"
                elif 'list' in cleaned_string and not cleaned_string.strip().startswith('{'):
                    # "list": [...] í˜•íƒœì¸ë° ì‹œì‘ì´ { ê°€ ì•„ë‹Œ ê²½ìš° (ì˜ëª»ëœ API)
                    # ì´ê±´ ë§¤ìš° ë³µì¡í•˜ë¯€ë¡œ, ì¼ë‹¨ì€ ìµœìƒìœ„ ê°ì²´ í˜•íƒœë§Œ ê³ ë ¤
                    pass

            # JSON ë‚´ì˜ ì˜ëª»ëœ ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„ë¥¼ ì œê±°í•˜ê±°ë‚˜ ì˜¬ë°”ë¥´ê²Œ ë³€í™˜.
            # ì˜ˆ: "abc\"def" (ì œëŒ€ë¡œ ì´ìŠ¤ì¼€ì´í”„ ì•ˆ ëœ ë”°ì˜´í‘œ) -> "abc\\"def" (ë”ë¸” ì´ìŠ¤ì¼€ì´í”„) ë˜ëŠ” "abc def" (ì œê±°)
            # ì—¬ê¸°ì„œëŠ” ê°€ì¥ í”í•œ íŒ¨í„´ì¸ ë¬¸ìì—´ ë‚´ì˜ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ ë”°ì˜´í‘œë¥¼ `\"`ë¡œ ë°”ê¾¸ëŠ” ì‹œë„
            # (ì´ê²ƒì€ ë§¤ìš° ì£¼ì˜í•´ì•¼ í•¨: `re.sub(r'(?<!\\)"', '\"', temp_string)`ì™€ ê°™ì´ í•˜ë©´ ê¸°ì¡´ `\"`ê¹Œì§€ `\\\"`ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ)
            # ê°€ì¥ ì•ˆì „í•œ ë°©ë²•ì€ `json.dumps(value)`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ì§€ë§Œ, ì´ëŠ” ì´ë¯¸ íŒŒì‹±ëœ ê°ì²´ì—ë§Œ ê°€ëŠ¥.
            # ê·¸ë˜ì„œ ì•„ë˜ì™€ ê°™ì´ íŒ¨í„´ì„ ì‚¬ìš©í•˜ë˜, íŠ¹ì • í‚¤ì—ë§Œ ì ìš©í•˜ëŠ” ê²ƒì´ ë” ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ì˜ˆ: "TP_ETC":"Some text with "unclosed quote""
            # ì´ íŒ¨í„´ì€ ë§¤ìš° ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì‹œë„ë¥¼ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # 1. í•„ë“œ ê°’ì´ HTML íƒœê·¸ë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°: `<[^>]*>` ì œê±°
            # 2. í•„ë“œ ê°’ì´ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ ë”°ì˜´í‘œë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°: í•´ë‹¹ ë”°ì˜´í‘œ ì•ì— ë°±ìŠ¬ë˜ì‹œ ì¶”ê°€

            # JSON ë¬¸ìì—´ ë‚´ì˜ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ " (ë”°ì˜´í‘œ)ë¥¼ ì°¾ì•„ì„œ ì´ìŠ¤ì¼€ì´í”„ (ë§¤ìš° ìœ„í—˜í•œ ì‹œë„ì¼ ìˆ˜ ìˆìŒ)
            # cleaned_string = re.sub(r'(?<!\\)"(?![,:}\]])', r'\"', cleaned_string)
            # ìœ„ íŒ¨í„´ì€ ì˜¤ë¥˜ë¥¼ ìœ ë°œí•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ. ëŒ€ì‹ , JSON íŒŒì„œê°€ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ë¬¸ìì—´ ë‚´ì˜ ì œì–´ ë¬¸ìë¥¼ ì œê±°.
            # (ì´ ë¶€ë¶„ì€ ìœ„ì—ì„œ [\x00-\x1f] ì²˜ë¦¬ë¡œ ì´ë¯¸ ì‹œë„ë¨)

            # ë‹¤ì‹œ json.loads ì‹œë„
            data = json.loads(cleaned_string)  # ìµœì¢…ì ìœ¼ë¡œ ë³´ì •ëœ ë¬¸ìì—´ë¡œ íŒŒì‹± ì‹œë„
            return data

        except json.JSONDecodeError as final_e:
            logging.error(f"âŒ ìµœì¢… JSON íŒŒì‹± ì‹¤íŒ¨: {final_e}. ì›ë³¸ ë¬¸ìì—´ì˜ ì¼ë¶€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤: {cleaned_string[:500]}...")
            return {
                "list": [],
                "totalCount": 0,
                "result": "ERROR",
                "resultMsg": f"ìµœì¢… íŒŒì‹± ì˜¤ë¥˜: {str(final_e)}"
            }
        except Exception as unknown_e:
            logging.error(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ íŒŒì‹± ì˜¤ë¥˜: {unknown_e}")
            return {
                "list": [],
                "totalCount": 0,
                "result": "ERROR",
                "resultMsg": f"ì˜ˆê¸°ì¹˜ ì•Šì€ íŒŒì‹± ì˜¤ë¥˜: {str(unknown_e)}"
            }

    def process_raw_data(self) -> List[Dict]:
        """ì €ì¥ëœ ì›ì²œ ë°ì´í„°ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•˜ì—¬ ì •ì œëœ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        raw_files = glob.glob(os.path.join(self.raw_data_dir, f"{self.date_str}_*_raw.json"))

        if not raw_files:
            logging.warning(f"âš ï¸ ì²˜ë¦¬í•  ì›ì²œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.raw_data_dir}")
            return []

        all_processed_data = []
        successful_files = 0

        logging.info(f"ğŸ”„ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {len(raw_files)}ê°œ íŒŒì¼")

        for file_path in raw_files:
            try:
                logging.info(f"ğŸ“ ì²˜ë¦¬ ì¤‘: {os.path.basename(file_path)}")

                # ì›ì²œ ë°ì´í„° ë¡œë“œ
                with open(file_path, 'r', encoding='utf-8') as f:
                    raw_data = json.load(f)

                # API ì‘ë‹µ íŒŒì‹±
                parsed_data = self.parse_api_response(raw_data['raw_response'])

                # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ì²˜ë¦¬
                if 'list' in parsed_data and isinstance(parsed_data['list'], list):
                    items = parsed_data['list']

                    # ê° ì•„ì´í…œì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    for item in items:
                        item.update({
                            'popup_category_main': raw_data['category_main'],
                            'popup_category_sub': raw_data['category_sub'],
                            'popup_code': raw_data['popup_code']
                        })

                    all_processed_data.extend(items)
                    successful_files += 1

                    logging.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {len(items)}ê°œ ì•„ì´í…œ ì¶”ê°€")
                else:
                    logging.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„° êµ¬ì¡°: {os.path.basename(file_path)}")

            except Exception as e:
                logging.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({os.path.basename(file_path)}): {e}")

        logging.info(f"ğŸ‰ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {successful_files}/{len(raw_files)}ê°œ íŒŒì¼, ì´ {len(all_processed_data)}ê°œ ì•„ì´í…œ")
        return all_processed_data

    def save_processed_data(self, processed_data: List[Dict]) -> bool:
        """ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        if not processed_data:
            logging.info("ì €ì¥í•  ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        try:
            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
            grouped_data = {}
            for item in processed_data:
                popup_code = item.get('popup_code', 'unknown')
                if popup_code not in grouped_data:
                    grouped_data[popup_code] = []
                grouped_data[popup_code].append(item)

            # ê° ê·¸ë£¹ì„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
            for popup_code, items in grouped_data.items():
                filename = os.path.join(self.processed_data_dir, f"{self.date_str}_{popup_code}_processed.json")

                processed_file_data = {
                    'popup_code': popup_code,
                    'processed_at': datetime.utcnow().isoformat() + 'Z',
                    'count': len(items),
                    'data': items
                }

                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(processed_file_data, f, ensure_ascii=False, indent=2)

                logging.info(f"âœ… ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥: {filename} ({len(items)}ê°œ ì•„ì´í…œ)")

            return True

        except Exception as e:
            logging.error(f"âŒ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def _remove_full_duplicates(self, data_list: List[Dict]) -> List[Dict]:
        """
        ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  í•„ë“œì˜ ê°’ì´ ë™ì¼í•œ ì™„ì „ ì¤‘ë³µ ë ˆì½”ë“œë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        """
        seen_records = set()
        unique_data = []
        original_count = len(data_list)

        for record in data_list:
            # ë”•ì…”ë„ˆë¦¬ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í•´ì‹œ ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
            # í‚¤ ìˆœì„œë¥¼ ì •ë ¬í•˜ì—¬ ê°™ì€ ë‚´ìš©ì´ë¼ë„ ìˆœì„œê°€ ë‹¤ë¥´ë©´ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ì¸ì‹í•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
            try:
                record_str = json.dumps(record, sort_keys=True, ensure_ascii=False)
            except TypeError as e:
                logging.warning(
                    f"âš ï¸ ë ˆì½”ë“œ ì§ë ¬í™” ì‹¤íŒ¨ (ì¤‘ë³µ ì œê±° ê±´ë„ˆë›°ê¸°): {e} - {record.get('TP_NAME', 'Unknown Product')}. ì´ ë ˆì½”ë“œëŠ” ì¤‘ë³µ ì œê±° ëŒ€ìƒì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.")
                # ì§ë ¬í™” ì‹¤íŒ¨ ì‹œ, í•´ë‹¹ ë ˆì½”ë“œëŠ” ê³ ìœ í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¶”ê°€í•©ë‹ˆë‹¤.
                unique_data.append(record)
                continue

            if record_str not in seen_records:
                seen_records.add(record_str)
                unique_data.append(record)
            else:
                logging.info(f"ğŸ§¹ ì™„ì „ ì¤‘ë³µ ë ˆì½”ë“œ ì œê±°: {record.get('TP_CODE', 'N/A')}, {record.get('TP_PAY_NAME', 'N/A')}")

        logging.info(f"âœ… ì „ì²´ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {original_count}ê°œ ë ˆì½”ë“œ ì¤‘ {len(unique_data)}ê°œ ìœ ë‹ˆí¬")
        return unique_data

    def store_in_supabase(self, processed_data: List[Dict]) -> bool:
        """ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë‹¤ë¥¸ í…Œì´ë¸”ê³¼ ì œì•½ ì¡°ê±´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""
        if not processed_data:
            logging.info("DBì— ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # Supabase ì €ì¥ ì „, ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ì™„ì „ ì¤‘ë³µ ì œê±° ìˆ˜í–‰
        logging.info(f"âœ¨ Supabase ì €ì¥ ì „, ì „ì²´ {len(processed_data)}ê°œ ë ˆì½”ë“œì—ì„œ ì™„ì „ ì¤‘ë³µ ì œê±° ì‹œì‘...")
        processed_data = self._remove_full_duplicates(processed_data)
        logging.info(f"âœ¨ ì™„ì „ ì¤‘ë³µ ì œê±° í›„ {len(processed_data)}ê°œ ë ˆì½”ë“œ ì €ì¥ ì‹œë„")

        total_overall_success = True
        current_time = datetime.utcnow().isoformat() + 'Z'

        # ë°ì´í„°ë¥¼ table_infoì— ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”
        grouped_by_table_type = {}
        for item in processed_data:
            # popup_codeì˜ ì• ë‘ê¸€ìë¥¼ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” íƒ€ì…ì„ ê²°ì •
            popup_code_prefix = item.get('popup_code', '')[:2]
            if popup_code_prefix in self.table_info:
                if popup_code_prefix not in grouped_by_table_type:
                    grouped_by_table_type[popup_code_prefix] = []
                grouped_by_table_type[popup_code_prefix].append(item)
            else:
                logging.warning(
                    f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” popup_code ì ‘ë‘ì‚¬ '{popup_code_prefix}'. ì´ ë°ì´í„°ëŠ” ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {item.get('TP_NAME')}")
                total_overall_success = False

        for table_type, items_for_table in grouped_by_table_type.items():
            table_name = self.table_info[table_type]['table_name']
            on_conflict_cols_str = self.table_info[table_type]['on_conflict']
            on_conflict_cols_list = [col.strip() for col in on_conflict_cols_str.split(',')]

            logging.info(f"ğŸ’¾ '{table_name}' í…Œì´ë¸”ì— ì €ì¥ ì‹œì‘: ì´ {len(items_for_table)}ê°œ ì•„ì´í…œ")

            batch_size = 100
            table_success_count = 0

            # ë°°ì¹˜ë¥¼ ë‚˜ëˆ ì„œ ì²˜ë¦¬
            for i in range(0, len(items_for_table), batch_size):
                batch = items_for_table[i:i + batch_size]

                # ê° ë ˆì½”ë“œì— updated_at ì¶”ê°€
                # on_conflict_colsë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œ ì¤‘ë³µ ì œê±°ëŠ” _remove_full_duplicatesê°€ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ,
                # ì—¬ê¸°ì„œëŠ” on_conflict ì»¬ëŸ¼ì˜ None ê°’ ì²˜ë¦¬ë§Œ ì‹ ê²½ì“°ë©´ ë©ë‹ˆë‹¤.
                # ê·¸ëŸ¬ë‚˜ _remove_full_duplicatesê°€ ëª¨ë“  í•„ë“œë¥¼ JSONí™”í•˜ë¯€ë¡œ,
                # ì—¬ê¸°ì„œëŠ” on_conflict í•„ë“œ ê°’ì— Noneì´ ìˆëŠ”ì§€ ë‹¤ì‹œ ê²€ì‚¬í•  í•„ìš”ëŠ” í¬ê²Œ ì—†ìŠµë‹ˆë‹¤.
                # (ë§Œì•½ on_conflict í•„ë“œ ìì²´ê°€ JSON ì§ë ¬í™”ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤ë©´).

                # ì‹¤ì œ Supabase UPSERTë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
                final_batch_for_supabase = []
                for item in batch:
                    # on_conflict ì»¬ëŸ¼ì— Noneì´ ìˆìœ¼ë©´ Supabaseì—ì„œ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±
                    # ë”°ë¼ì„œ ì´ ë¶€ë¶„ì—ì„œ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•˜ê³ , í•„ìš”ì‹œ ëŒ€ì²´ ê°’ ì ìš©
                    valid_item = True
                    for col in on_conflict_cols_list:
                        if item.get(col) is None:
                            logging.warning(
                                f"âŒ '{table_name}' í…Œì´ë¸”ì˜ on_conflict ì»¬ëŸ¼ '{col}'ì— None ê°’ ë°œê²¬. "
                                f"ë ˆì½”ë“œ ìŠ¤í‚µ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´: {item.get('TP_NAME', 'Unknown')}"
                            )
                            # None ê°’ì„ í¬í•¨í•œ ë ˆì½”ë“œë¥¼ ê±´ë„ˆë›¸ì§€, ë¹ˆ ë¬¸ìì—´ ë“±ìœ¼ë¡œ ëŒ€ì²´í• ì§€ ê²°ì •
                            # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ìŠ¤í‚µí•˜ì§€ ì•Šê³ , Supabaseê°€ Noneì„ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
                            # SupabaseëŠ” ON CONFLICTì—ì„œ NULL ê°’ì„ í—ˆìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                            # ë§Œì•½ ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤ë©´, ì´ ë¶€ë¶„ì—ì„œ None ê°’ì„ "" ë“±ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ë¡œì§ ì¶”ê°€ í•„ìš”.
                            # ì˜ˆ: item[col] = "" if item.get(col) is None else item[col]
                            pass  # í˜„ì¬ëŠ” ìŠ¤í‚µí•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì „ë‹¬

                    final_batch_for_supabase.append({**item, 'updated_at': current_time})

                if not final_batch_for_supabase:
                    logging.info(f"âœ… ë°°ì¹˜ {i // batch_size + 1}ì— Supabaseì— ì €ì¥í•  ìœ íš¨í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆë›°ê¸°.")
                    continue

                try:
                    # Bulk UPSERT ì‹¤í–‰
                    response = self.supabase.table(table_name).upsert(
                        final_batch_for_supabase,
                        on_conflict=on_conflict_cols_str  # ë™ì ìœ¼ë¡œ on_conflict ì„¤ì •
                    ).execute()

                    batch_success = len(response.data) if response.data else 0
                    table_success_count += batch_success

                    logging.info(
                        f"âœ… '{table_name}' ë°°ì¹˜ ì €ì¥ ({i // batch_size + 1}/{(len(items_for_table) + batch_size - 1) // batch_size}): "
                        f"{batch_success}/{len(final_batch_for_supabase)}ê°œ ì„±ê³µ"
                    )

                except Exception as e:
                    logging.error(f"âŒ '{table_name}' ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨ (ë°°ì¹˜ {i // batch_size + 1}): {e}")
                    total_overall_success = False

                    # ì‹¤íŒ¨ ì‹œ ê°œë³„ ë ˆì½”ë“œë¡œ ì¬ì‹œë„ (ì¤‘ë³µ ì œê±°ëœ ë°°ì¹˜ì— ëŒ€í•´ì„œë§Œ)
                    logging.info("ê°œë³„ ë ˆì½”ë“œë¡œ ì¬ì‹œë„ ì¤‘...")
                    for record in final_batch_for_supabase:  # deduplicated_batch ëŒ€ì‹  final_batch_for_supabase ì‚¬ìš©
                        try:
                            individual_response = self.supabase.table(table_name).upsert(
                                {**record},  # ì´ë¯¸ updated_at ì¶”ê°€ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                                on_conflict=on_conflict_cols_str  # ë™ì ìœ¼ë¡œ on_conflict ì„¤ì •
                            ).execute()

                            if individual_response.data:
                                table_success_count += 1
                                logging.info(f"âœ… ê°œë³„ ì €ì¥ ì„±ê³µ: {record.get('TP_NAME', 'Unknown')}")

                        except Exception as individual_e:
                            logging.error(f"âŒ ê°œë³„ ì €ì¥ ì‹¤íŒ¨: {record.get('TP_NAME', 'Unknown')} - {individual_e}")
                            total_overall_success = False

            logging.info(f"ğŸ‰ '{table_name}' DB ì €ì¥ ì™„ë£Œ: {table_success_count}/{len(items_for_table)}ê°œ ì„±ê³µ")

        if not grouped_by_table_type:
            logging.warning("âš ï¸ ì €ì¥í•  í…Œì´ë¸” ìœ í˜•ì´ ì •ì˜ë˜ì§€ ì•Šì€ ë°ì´í„°ê°€ ë§ê±°ë‚˜, ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        return total_overall_success

    def cleanup_old_files(self, days_to_keep: int = 7):
        """ì§€ì •ëœ ì¼ìˆ˜ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
        try:
            cutoff_date = datetime.now().timestamp() - (days_to_keep * 24 * 60 * 60)

            for directory in [self.raw_data_dir, self.processed_data_dir]:
                files = glob.glob(os.path.join(directory, "*.json"))
                deleted_count = 0

                for file_path in files:
                    if os.path.getmtime(file_path) < cutoff_date:
                        os.remove(file_path)
                        deleted_count += 1

                if deleted_count > 0:
                    logging.info(f"ğŸ§¹ ì •ë¦¬ ì™„ë£Œ ({directory}): {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ")

        except Exception as e:
            logging.error(f"âŒ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def run_full_pipeline(self, cleanup_days: int = 7):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤: ìˆ˜ì§‘ â†’ ì²˜ë¦¬ â†’ ì €ì¥ â†’ ì •ë¦¬"""
        pipeline_start_time = datetime.now() - timedelta(days=1)
        # ì˜¤ëŠ˜ ë‚ ì§œ
        self.today = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        self.date_str = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')

        logging.info(f"ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {pipeline_start_time.strftime('%Y-%m-%d %H:%M:%S')}")

        try:
            # # 1. ì›ì²œ ë°ì´í„° ìˆ˜ì§‘
            # logging.info("=" * 50)
            # logging.info("1ë‹¨ê³„: ì›ì²œ ë°ì´í„° ìˆ˜ì§‘")
            # logging.info("=" * 50)
            #
            # if not self.collect_raw_data():
            #     logging.error("âŒ ì›ì²œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            #     return False

            # # 2. ë°ì´í„° ì²˜ë¦¬
            # logging.info("=" * 50)
            # logging.info("2ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬ ë° ë³´ì •")
            # logging.info("=" * 50)
            #
            processed_data = self.process_raw_data()
            if not processed_data:
                logging.error("âŒ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
                return False

            # 3. ì²˜ë¦¬ëœ ë°ì´í„° ë¡œì»¬ ì €ì¥
            logging.info("=" * 50)
            logging.info("3ë‹¨ê³„: ì²˜ë¦¬ëœ ë°ì´í„° ë¡œì»¬ ì €ì¥")
            logging.info("=" * 50)

            if not self.save_processed_data(processed_data):
                logging.warning("âš ï¸ ì²˜ë¦¬ëœ ë°ì´í„° ë¡œì»¬ ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")

            # 4. DB ì €ì¥
            logging.info("=" * 50)
            logging.info("4ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
            logging.info("=" * 50)

            if not self.store_in_supabase(processed_data):
                logging.error("âŒ DB ì €ì¥ ì‹¤íŒ¨")
                return False

            # 5. ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬
            if cleanup_days > 0:
                logging.info("=" * 50)
                logging.info("5ë‹¨ê³„: íŒŒì¼ ì •ë¦¬")
                logging.info("=" * 50)
                self.cleanup_old_files(cleanup_days)

            # ì™„ë£Œ
            pipeline_end_time = datetime.now()
            duration = pipeline_end_time - pipeline_start_time
            logging.info("=" * 50)
            logging.info(f"ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            logging.info(f"   ì†Œìš” ì‹œê°„: {duration}")
            logging.info(f"   ì²˜ë¦¬ëœ ë°ì´í„°: {len(processed_data)}ê°œ")
            logging.info("=" * 50)

            return True

        except Exception as e:
            logging.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logging.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return False

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì„œë“œë“¤
    def collect_daily_disclosures(self):
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ (ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)"""
        return self.run_full_pipeline()

    def run(self):
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ (ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)"""
        return self.run_full_pipeline()


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    collector = DisclosureCollector(
        raw_data_dir="raw_data",  # ì›ì²œ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        processed_data_dir="processed_data"  # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
    )

    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    success = collector.run_full_pipeline(cleanup_days=7)

    if success:
        print("âœ… ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")