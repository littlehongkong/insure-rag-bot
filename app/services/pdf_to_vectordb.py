#B00312010_1_P
import os
import re
import hashlib
import requests
from pathlib import Path
from datetime import datetime, timedelta, timezone
from typing import Dict, Any, Optional, List
import PyPDF2
import chromadb
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer
import supabase  # pip install supabase

# =========================
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# =========================
load_dotenv()


class PDFToVectorDB:
    """
    ë³´í—˜ ì•½ê´€ PDF -> í…ìŠ¤íŠ¸/í‘œ ì¶”ì¶œ -> ì²­í¬/ì„ë² ë”© -> ChromaDB ì €ì¥ íŒŒì´í”„ë¼ì¸
    - Supabaseì˜ insurance_products í…Œì´ë¸”ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ëŠ˜(updated_at) ë³€ê²½ëœ ì•½ê´€ë§Œ ì²˜ë¦¬
    - ì²˜ë¦¬ ì „ ê°™ì€ (insurer_code, item_code) ê¸°ì¡´ ë²¡í„°ëŠ” ì „ëŸ‰ ì‚­ì œí•˜ì—¬ ìµœì‹  ì•½ê´€ë§Œ ìœ ì§€
    """

    def __init__(self, insurer_code: str = "LINA", collection_name: str = "insurance_pdfs", use_openai: bool = False, openai_api_key: Optional[str] = None):
        self.insurer_code = insurer_code
        self.collection_name = collection_name

        self.use_openai = use_openai
        self.openai_api_key = openai_api_key
        self.embeddings = None  # (ì˜µì…˜) OpenAI ì“¸ ë•Œ í• ë‹¹

        # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ (384ì°¨ì›)
        self.embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        self.tokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sroberta-multitask')

        # ê¸¸ì´ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸° (í† í¬ë‚˜ì´ì € ê¸°ì¤€)
        self.length_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=50,
            length_function=lambda text: len(self.tokenizer.encode(text, add_special_tokens=False)),
            separators=["\n\n", "\n", " ", ""]
        )

        # ì €ì¥ ê²½ë¡œ
        base_path = Path(__file__).resolve().parent.parent.parent
        self.base_pdf_dir = base_path / 'pdfs'
        self.base_pdf_dir.mkdir(parents=True, exist_ok=True)
        self.pdf_dir = self.base_pdf_dir / self.insurer_code
        self.pdf_dir.mkdir(parents=True, exist_ok=True)

        # ChromaDB (ì˜êµ¬í˜•)
        db_path = base_path / 'chroma_db'
        db_path.mkdir(parents=True, exist_ok=True)
        self.chroma_client = chromadb.PersistentClient(path=str(db_path))
        try:
            self.collection = self.chroma_client.get_collection(self.collection_name)
        except Exception:
            self.collection = self.chroma_client.create_collection(name=self.collection_name)

        # Supabase
        self.supabase_url = os.getenv('SUPABASE_URL')
        self.supabase_key = os.getenv('SUPABASE_KEY')
        self.supabase = supabase.create_client(self.supabase_url, self.supabase_key)

    # =========================
    # ìœ í‹¸
    # =========================
    def _start_of_today_kst_iso(self) -> str:
        """KST(UTC+9) ì˜¤ëŠ˜ 00:00:00ì„ ISO8601ë¡œ ë°˜í™˜"""
        kst = timezone(timedelta(hours=9))
        now_kst = datetime.now(kst)
        start = datetime(year=now_kst.year, month=now_kst.month, day=now_kst.day, tzinfo=kst)
        return start.isoformat()

    def _deterministic_id(self, insurer_code: str, item_code: str, file_name: str, page: int, chunk_idx: int) -> str:
        raw = f"{insurer_code}|{item_code}|{file_name}|{page}|{chunk_idx}"
        return hashlib.md5(raw.encode("utf-8")).hexdigest()

    def _clean_text(self, text: str) -> str:
        return re.sub(r'[\x00\u0000]', '', text)

    def _regex_chunk_split(self, text: str) -> List[str]:
        """
        í‘œ/ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ chunkë¡œ ë¬¶ê¸° ìœ„í•œ regex ë¶„ë¦¬ (ì› ì½”ë“œ ê°œì„ /ì¬ì‚¬ìš©)
        """
        pattern = r"(\n\s*(?:[-â€¢*]|\d+\.)\s.*|\n.*\t.*)"
        parts = re.split(pattern, text)
        chunks = []
        buffer = ""
        for part in parts:
            if not part:
                continue
            if re.match(pattern, part):
                buffer += part
            else:
                if buffer:
                    chunks.append(buffer.strip())
                    buffer = ""
                chunks.append(part.strip())
        if buffer:
            chunks.append(buffer.strip())
        return [c for c in chunks if c]

    # =========================
    # Supabase ì¡°íšŒ
    # =========================
    def fetch_products_updated_today(self) -> List[Dict[str, Any]]:
        """
        ì˜¤ëŠ˜(KST) ì—…ë°ì´íŠ¸ëœ ë³´í—˜ ìƒí’ˆ(ì•½ê´€)ë§Œ ì¡°íšŒ
        ìŠ¤í‚¤ë§ˆ: insurance_products
        """
        start_kst = self._start_of_today_kst_iso()
        res = self.supabase.table('insurance_products') \
            .select('*') \
            .eq('insurer_code', self.insurer_code) \
            .order('updated_at', desc=False) \
            .execute()

        # .gte('updated_at', start_kst) \
        return res.data if hasattr(res, 'data') else []

    # =========================
    # PDF ë‹¤ìš´ë¡œë“œ
    # =========================
    def download_pdf_from_dbrow(self, row: Dict[str, Any]) -> Optional[str]:
        """
        DB ë ˆì½”ë“œì˜ file_url / file_name ê¸°ì¤€ìœ¼ë¡œ PDF ë‹¤ìš´ë¡œë“œ
        """
        file_url = row.get('file_url')
        file_name = row.get('file_name') or f"{row.get('item_code', 'unknown')}.pdf"
        if not file_url:
            print(f"[ê²½ê³ ] file_url ì´ ì—†ì–´ ë‹¤ìš´ë¡œë“œ ê±´ë„ˆëœ€: item_code={row.get('item_code')}")
            return None

        local_path = self.pdf_dir / file_name
        try:
            resp = requests.get(file_url, stream=True, timeout=30)
            resp.raise_for_status()
            with open(local_path, 'wb') as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    f.write(chunk)
            # ìœ íš¨ì„±
            with open(local_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                _ = len(reader.pages)
            print(f"[ë‹¤ìš´ë¡œë“œ ì™„ë£Œ] {local_path}")
            return str(local_path)
        except Exception as e:
            print(f"[ì˜¤ë¥˜] PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_url} ({e})")
            if local_path.exists():
                try:
                    local_path.unlink()
                except Exception:
                    pass
            return None

    # =========================
    # í…ìŠ¤íŠ¸ / í‘œ ì¶”ì¶œ
    # =========================
    def _build_metadata(self, base_meta: Dict[str, Any], pdf_path: str, page_num: int, is_table: bool) -> Dict[
        str, Any]:
        """ë©”íƒ€ë°ì´í„° ìƒì„± - í˜ì´ì§€ ë²ˆí˜¸ëŠ” 0-basedì—ì„œ 1-basedë¡œ ë³€í™˜"""
        md = {
            # íŒŒì¼/í˜ì´ì§€ (ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ 1-based í˜ì´ì§€ ë²ˆí˜¸)
            "source": str(pdf_path),
            "page": page_num,  # ë‚´ë¶€ì ìœ¼ë¡œëŠ” 0-based ìœ ì§€
            "page_display": page_num + 1,  # ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë•ŒëŠ” 1-based
            "is_table": is_table,

            # DB ê¸°ë³¸ ë©”íƒ€
            "insurer_code": base_meta.get("insurer_code"),
            "insurer_name": base_meta.get("insurer_name"),
            "item_type": base_meta.get("item_type"),
            "category_name": base_meta.get("category_name"),
            "item_name": base_meta.get("item_name"),
            "item_code": base_meta.get("item_code"),
            "file_name": base_meta.get("file_name"),
            "file_url": base_meta.get("file_url"),
            "status": base_meta.get("status"),

            # details í™•ì¥
            "sell_open_date": (base_meta.get("details") or {}).get("sell_open_date"),
            "sell_end_date": (base_meta.get("details") or {}).get("sell_end_date"),
            "klia_product_code": (base_meta.get("details") or {}).get("klia_product_code"),
            "kcis_insurance_code": (base_meta.get("details") or {}).get("kcis_insurance_code"),
            "prod_pban_grp_cd": (base_meta.get("details") or {}).get("prod_pban_grp_cd"),
            "product_summary": (base_meta.get("details") or {}).get("product_summary"),
            "product_method": (base_meta.get("details") or {}).get("product_method"),
            "item_sequence": (base_meta.get("details") or {}).get("item_sequence"),
            "item_section": (base_meta.get("details") or {}).get("item_section"),
            "crawled_at": (base_meta.get("details") or {}).get("crawled_at"),
            "api_version": (base_meta.get("details") or {}).get("api_version"),

            # ì¸ë±ì‹± ì‹œì 
            "indexed_at": datetime.now(timezone.utc).isoformat()
        }
        return md

    def extract_documents(self, pdf_path: str, db_row: Dict[str, Any]) -> List[Document]:
        import fitz  # PyMuPDF
        import pdfplumber

        docs: List[Document] = []

        # PyMuPDFë¡œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì •í™•í•œ í˜ì´ì§€ ë§¤í•‘)
        try:
            fitz_doc = fitz.open(pdf_path)

            for page_num, page in enumerate(fitz_doc):
                page_text = page.get_text("text").strip()

                if page_text:
                    md = self._build_metadata(db_row, pdf_path, page_num, is_table=False)
                    docs.append(Document(
                        page_content=self._clean_text(page_text),
                        metadata=md
                    ))

            fitz_doc.close()
            print(f"[ì¶”ì¶œ] PyMuPDFë¡œ {len(docs)}ê°œ í˜ì´ì§€ ì²˜ë¦¬")

        except Exception as e:
            print(f"[ERROR] PyMuPDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

        # í…Œì´ë¸” ë°ì´í„°ëŠ” ë³„ë„ë¡œ pdfplumberë¡œ ì¶”ì¶œ (í˜ì´ì§€ë³„ ì •í™• ë§¤í•‘)
        try:
            with pdfplumber.open(pdf_path) as plumber_pdf:
                for page_num, page in enumerate(plumber_pdf.pages):
                    tables = page.extract_tables()

                    for table_idx, table in enumerate(tables or []):
                        if not table or not any(any(cell for cell in row if cell) for row in table):
                            continue

                        # í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        table_rows = []
                        for row in table:
                            cleaned_row = [str(cell).strip() if cell else "" for cell in row]
                            if any(cleaned_row):  # ë¹ˆ í–‰ ì œì™¸
                                table_rows.append(" | ".join(cleaned_row))

                        if table_rows:
                            table_text = f"[TABLE {table_idx + 1}]\n" + "\n".join(table_rows)
                            md = self._build_metadata(db_row, pdf_path, page_num, is_table=True)
                            # í…Œì´ë¸” ë©”íƒ€ë°ì´í„°ì— í…Œì´ë¸” ë²ˆí˜¸ ì¶”ê°€
                            md["table_index"] = table_idx

                            docs.append(Document(
                                page_content=self._clean_text(table_text),
                                metadata=md
                            ))

            table_count = sum(1 for doc in docs if doc.metadata.get("is_table"))
            print(f"[ì¶”ì¶œ] pdfplumberë¡œ {table_count}ê°œ í…Œì´ë¸” ì²˜ë¦¬")

        except Exception as e:
            print(f"[WARN] í…Œì´ë¸” ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path} â†’ {e}")

        return docs


    # =========================
    # ì„ë² ë”©
    # =========================
    def create_embeddings(self, texts: List[str]):
        if self.use_openai:
            # í•„ìš” ì‹œ OpenAI ì„ë² ë”©ìœ¼ë¡œ êµì²´ (í˜„ì¬ëŠ” sentence_transformers ì‚¬ìš©)
            return [self.embeddings.embed_query(t) for t in texts]
        return self.embedding_model.encode(texts, normalize_embeddings=True)

    # =========================
    # ì €ì¥(Chroma) â€” ê¸°ì¡´ ë²¡í„° ì‚­ì œ í›„ ì—…ì„œíŠ¸
    # =========================
    def upsert_documents(self, docs: List[Document], insurer_code: str, item_code: str, file_name: str):
        if not docs:
            print("[ê²½ê³ ] ì €ì¥í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # 1) ì´ì „ ë°ì´í„° ì‚­ì œ: ê°™ì€ (insurer_code, item_code) ì „ëŸ‰ ì œê±° â†’ ê°œì • ì „ ë²¡í„° ì œê±°
        try:
            self.collection.delete(where={"insurer_code": insurer_code, "item_code": item_code})
            print(f"[ì •ë¦¬] ê¸°ì¡´ ë²¡í„° ì‚­ì œ ì™„ë£Œ: insurer_code={insurer_code}, item_code={item_code}")
        except Exception as e:
            print(f"[ê²½ê³ ] ê¸°ì¡´ ë²¡í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")

        # 2) ì²­í¬í™”
        chunks: List[Document] = []
        for d in docs:
            # í‘œ/ë¦¬ìŠ¤íŠ¸ë¥¼ ìš°ì„  ë¸”ë¡ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
            blocks = self._regex_chunk_split(d.page_content)
            for block in blocks:
                # ê¸¸ì´ ê¸°ë°˜ ì„¸ë¶„í™”
                sub_texts = self.length_splitter.split_text(block)
                for sub in sub_texts:
                    chunks.append(Document(page_content=sub, metadata=d.metadata))

        print(f"[ë¶„í• ] ì´ {len(chunks)}ê°œ ì²­í¬")

        # 3) ì„ë² ë”© ë° ì—…ì„œíŠ¸ (ê²°ì •ì  ID)
        texts = [c.page_content for c in chunks]
        metas = [c.metadata for c in chunks]
        embeddings = self.create_embeddings(texts)

        ids = []
        per_page_counters = {}  # í˜ì´ì§€ë³„ ì²­í¬ ì¸ë±ìŠ¤
        for m in metas:
            page = int(m.get("page", 0))
            key = (page,)
            per_page_counters[key] = per_page_counters.get(key, 0) + 1
            chunk_idx = per_page_counters[key] - 1
            # ë©”íƒ€ë°ì´í„°ì— chunk_index ì¶”ê°€
            m['chunk_index'] = chunk_idx
            ids.append(self._deterministic_id(insurer_code, item_code, file_name, page, chunk_idx))

        # ë°°ì¹˜ ì—…ì„œíŠ¸
        BATCH = 64
        try:
            for i in range(0, len(chunks), BATCH):
                self.collection.upsert(
                    documents=texts[i:i+BATCH],
                    embeddings=embeddings[i:i+BATCH],
                    metadatas=metas[i:i+BATCH],
                    ids=ids[i:i+BATCH]
                )
                print(f"[ì—…ì„œíŠ¸] {min(i+BATCH, len(chunks))}/{len(chunks)}")
            print(f"[ì™„ë£Œ] ì´ {len(chunks)}ê°œ ì²­í¬ ì—…ì„œíŠ¸ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"[ì˜¤ë¥˜] Chroma ì—…ì„œíŠ¸ ì‹¤íŒ¨: {e}")
            return False

    # =========================
    # ë©”ì¸ ì²˜ë¦¬ ë£¨í‹´
    # =========================
    def index_today_updated_products(self):
        """
        ì˜¤ëŠ˜ ë³€ê²½ëœ ì•½ê´€ë§Œ ìƒ‰ì¸
        """
        rows = self.fetch_products_updated_today()
        print(f"[ì¡°íšŒ] ì˜¤ëŠ˜ ë³€ê²½ëœ ì•½ê´€: {len(rows)}ê±´")
        if not rows:
            return

        for idx, row in enumerate(rows, 1):
            try:
                print(f"\n[{idx}/{len(rows)}] {row.get('item_name')} ({row.get('item_code')}) - {row.get('file_name')}")
                pdf_path = self.pdf_dir / row.get('file_name')
                # pdf_path = self.download_pdf_from_dbrow(row)
                # if not pdf_path:
                #     print("[ê±´ë„ˆëœ€] PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                #     continue

                docs = self.extract_documents(pdf_path, db_row=row)
                if not docs:
                    print("[ê±´ë„ˆëœ€] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨/ì—†ìŒ")
                    continue

                ok = self.upsert_documents(
                    docs,
                    insurer_code=row.get("insurer_code"),
                    item_code=row.get("item_code"),
                    file_name=row.get("file_name")
                )
                if ok:
                    print(f"[ìƒ‰ì¸ ì™„ë£Œ] {row.get('item_code')} / {row.get('file_name')}")
                else:
                    print(f"[ì‹¤íŒ¨] ìƒ‰ì¸ ì‹¤íŒ¨: {row.get('item_code')} / {row.get('file_name')}")
            except Exception as e:
                print(f"[ì˜ˆì™¸] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    def search_with_context(self, query: str, k: int = 5,
                            context_window: int = 1) -> List[Dict[str, Any]]:
        """
        ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¥¼ í¬í•¨í•œ ê²€ìƒ‰
        - ê²€ìƒ‰ëœ ì²­í¬ì˜ ì•ë’¤ ì²­í¬ë„ í•¨ê»˜ ë°˜í™˜í•˜ì—¬ ë¬¸ë§¥ ì´í•´ í–¥ìƒ
        """
        base_results = self.search(query, k=k)

        if not base_results.get("documents") or not base_results["documents"][0]:
            return []

        enriched_results = []

        for doc, meta, distance in zip(
                base_results["documents"][0],
                base_results["metadatas"][0],
                base_results.get("distances", [[]])[0]
        ):
            # ê¸°ë³¸ ê²°ê³¼
            result_item = {
                "content": doc,
                "metadata": meta,
                "similarity": 1 - distance,
                "context_chunks": []
            }

            # ê°™ì€ í˜ì´ì§€ì˜ ì•ë’¤ ì²­í¬ ê²€ìƒ‰
            try:
                context_results = self.collection.query(
                    query_embeddings=[],  # ì„ë² ë”© ê²€ìƒ‰ ì—†ì´
                    n_results=50,  # ì¶©ë¶„í•œ ìˆ˜
                    where={
                        "insurer_code": meta.get("insurer_code"),
                        "item_code": meta.get("item_code"),
                        "page": meta.get("page")
                    }
                )

                # ë¸”ë¡ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if context_results.get("metadatas"):
                    current_block = meta.get("chunk_index", 0)
                    context_candidates = []

                    for ctx_doc, ctx_meta in zip(
                            context_results["documents"][0],
                            context_results["metadatas"][0]
                    ):
                        ctx_block = ctx_meta.get("chunk_index", 999)
                        if abs(ctx_block - current_block) <= context_window:
                            context_candidates.append({
                                "content": ctx_doc,
                                "metadata": ctx_meta,
                                "block_distance": abs(ctx_block - current_block)
                            })

                    # ë¸”ë¡ ê±°ë¦¬ìˆœ ì •ë ¬
                    context_candidates.sort(key=lambda x: x["block_distance"])
                    result_item["context_chunks"] = context_candidates[:context_window * 2 + 1]

            except Exception as e:
                print(f"[WARN] ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

            enriched_results.append(result_item)

        return enriched_results

    def test_queries(self):
        """
        ìë™í™”ëœ ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
        """
        test_cases = {
            # ì¼ë°˜ì ì¸ ë³´í—˜ ê´€ë ¨ ì§ˆë¬¸ë“¤
            "ì…ì›ì‹œ ì‹ëŒ€ëŠ” ë³´ì¥ë˜ë‚˜ìš”?": ["ì‹ëŒ€", "ì…ì›", "ë³´ì¥"],
            "ì‚¬ë§ì‹œ ëˆ„ê°€ ë³´í—˜ê¸ˆì„ ë°›ë‚˜ìš”?": ["ì‚¬ë§", "ë³´í—˜ê¸ˆ", "ìˆ˜ìµì"],
            "ê°±ì‹  ì‹œ ë³´í—˜ë£Œê°€ ì˜¤ë¥´ë‚˜ìš”?": ["ê°±ì‹ ", "ë³´í—˜ë£Œ", "ì¸ìƒ"],
            "ë©´ì±…ê¸°ê°„ì€ ì–¸ì œê¹Œì§€ì¸ê°€ìš”?": ["ë©´ì±…", "ê¸°ê°„"],
            "í•´ì•½í™˜ê¸‰ê¸ˆì€ ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?": ["í•´ì•½", "í™˜ê¸‰ê¸ˆ", "ê³„ì‚°"],
            "ë³´í—˜ë£Œ ë‚©ì…ì´ ì—°ì²´ë˜ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?": ["ë³´í—˜ë£Œ", "ì—°ì²´", "ë‚©ì…"],
        }

        print("ğŸ§ª ê²€ìƒ‰ í’ˆì§ˆ ìë™í™” í…ŒìŠ¤íŠ¸")
        print("=" * 60)

        total_tests = len(test_cases)
        passed_tests = 0

        for query, expected_keywords in test_cases.items():
            print(f"\nâ“ ì§ˆë¬¸: {query}")

            # ê²€ìƒ‰ ì‹¤í–‰
            results = self.search(query, k=3, debug=True)

            if not results.get("documents") or not results["documents"][0]:
                print("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                continue

            # ìƒìœ„ ê²°ê³¼ í‰ê°€
            top_result = results["documents"][0][0]
            top_meta = results["metadatas"][0][0]

            # í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ì‚¬
            matched_keywords = []
            for keyword in expected_keywords:
                if keyword in top_result:
                    matched_keywords.append(keyword)

            # ê²°ê³¼ í‰ê°€
            success_rate = len(matched_keywords) / len(expected_keywords)
            is_pass = success_rate >= 0.5  # 50% ì´ìƒ í‚¤ì›Œë“œ ë§¤ì¹­ì‹œ í†µê³¼

            if is_pass:
                passed_tests += 1
                print(f"âœ… PASS (í‚¤ì›Œë“œ ë§¤ì¹­: {len(matched_keywords)}/{len(expected_keywords)})")
            else:
                print(f"âŒ FAIL (í‚¤ì›Œë“œ ë§¤ì¹­: {len(matched_keywords)}/{len(expected_keywords)})")

            print(f"ğŸ“„ ì¶œì²˜: {top_meta.get('item_name')} p.{top_meta.get('page_display')}")
            print(f"ğŸ’¡ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {top_result[:100]}...")

            if matched_keywords:
                print(f"ğŸ¯ ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(matched_keywords)}")

        # ì „ì²´ ê²°ê³¼
        print("\n" + "=" * 60)
        print(f"ğŸ† ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed_tests}/{total_tests} ({passed_tests / total_tests * 100:.1f}%)")

        if passed_tests / total_tests >= 0.7:
            print("ğŸ‰ ê²€ìƒ‰ í’ˆì§ˆ ì–‘í˜¸!")
        else:
            print("âš ï¸  ê²€ìƒ‰ í’ˆì§ˆ ê°œì„  í•„ìš”")

        return passed_tests / total_tests

    # PDFToVectorDB í´ë˜ìŠ¤ ë‚´ë¶€ì— ì•„ë˜ í•¨ìˆ˜ ì¶”ê°€
    # =========================
    # ìœ í‹¸
    # =========================
    def _normalize_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
        """
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜, íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë“±ì„ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì˜ˆì‹œ: return re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text).lower()
        return text.strip()


    # =========================
    # ê²€ìƒ‰ ìœ í‹¸ (í…ŒìŠ¤íŠ¸ìš©)
    # =========================
    def search(self, query: str, k: int = 3,
               insurer_code: Optional[str] = None,
               item_code: Optional[str] = None,
               include_tables: bool = True,
               debug: bool = False) -> Dict[str, Any]:
        """
        í–¥ìƒëœ ê²€ìƒ‰ ê¸°ëŠ¥

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            insurer_code: ë³´í—˜ì‚¬ ì½”ë“œ í•„í„°
            item_code: ìƒí’ˆ ì½”ë“œ í•„í„°
            include_tables: í…Œì´ë¸” ë°ì´í„° í¬í•¨ ì—¬ë¶€
            debug: ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥ ì—¬ë¶€
        """
        # ì¿¼ë¦¬ ì •ê·œí™”
        normalized_query = self._normalize_text(query)
        q_emb = self.create_embeddings([normalized_query])[0]

        # where ì¡°ê±´ êµ¬ì„±
        where_conditions = {}
        if insurer_code:
            where_conditions["insurer_code"] = insurer_code
        if item_code:
            where_conditions["item_code"] = item_code
        if not include_tables:
            where_conditions["is_table"] = False

        # ê²€ìƒ‰ ì‹¤í–‰
        try:
            if where_conditions:
                result = self.collection.query(
                    query_embeddings=[q_emb],
                    n_results=k,
                    where=where_conditions
                )
            else:
                result = self.collection.query(
                    query_embeddings=[q_emb],
                    n_results=k
                )

            # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
            if debug:
                print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
                print(f"ğŸ“Š ì •ê·œí™”ëœ ì¿¼ë¦¬: '{normalized_query}'")
                print(f"ğŸ¯ í•„í„° ì¡°ê±´: {where_conditions}")
                print(f"ğŸ“ˆ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(result.get('documents', [[]])[0])}")
                print("=" * 50)

                for i, (doc, meta, distance) in enumerate(zip(
                        result.get("documents", [[]])[0],
                        result.get("metadatas", [[]])[0],
                        result.get("distances", [[]])[0]
                )):
                    print(f"\n[{i + 1}] ìœ ì‚¬ë„: {1 - distance:.3f}")
                    print(f"ğŸ“„ {meta.get('item_name')} ({meta.get('item_code')})")
                    print(
                        f"ğŸ“„ í˜ì´ì§€: {meta.get('page_display', meta.get('page', 0))} {'[í…Œì´ë¸”]' if meta.get('is_table') else '[í…ìŠ¤íŠ¸]'}")
                    print(f"ğŸ“„ íŒŒì¼: {meta.get('file_name')}")
                    print(f"ğŸ’¡ ë‚´ìš©: {doc[:200]}{'...' if len(doc) > 200 else ''}")
                    print("-" * 30)

            return result

        except Exception as e:
            print(f"[ERROR] ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {"documents": [[]], "metadatas": [[]], "distances": [[]]}


def main():
    processor = PDFToVectorDB(insurer_code="LINA", collection_name="insurance_pdfs", use_openai=False)
    print("[ì‹œì‘] ì˜¤ëŠ˜ ë³€ê²½ë¶„ ìƒ‰ì¸")
    processor.index_today_updated_products()
    print("[ë] ìƒ‰ì¸ ì™„ë£Œ")

    # ê°„ë‹¨ ê²€ìƒ‰ ì˜ˆì‹œ
    try:
        res = processor.search("ê°±ì‹ /ë¹„ê°±ì‹  ì¡°ê±´ê³¼ ë©´ì±…ì‚¬í•­ì„ ì•Œë ¤ì¤˜", k=3)
        if res and res.get("documents"):
            print("\n=== ê²€ìƒ‰ ê²°ê³¼ ===")
            for i, (doc, meta) in enumerate(zip(res["documents"][0], res["metadatas"][0]), 1):
                print(f"{i}. {meta.get('item_name')} ({meta.get('item_code')}) p.{int(meta.get('page',0))+1}  [{meta.get('file_name')}]")
                print(doc[:200] + ("..." if len(doc) > 200 else ""))
    except Exception as e:
        print(f"[ê²€ìƒ‰ ì˜ˆì™¸] {e}")


if __name__ == "__main__":
    main()
