import streamlit as st
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os

# Page configuration
st.set_page_config(
    page_title="ë³´í—˜ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Initialize chatbot
def init_chatbot():
    # Initialize embedding model
    embedding = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    # Initialize Chroma DB
    persist_directory = "./chroma_db"
    vectordb = Chroma(
        persist_directory=persist_directory,
        embedding_function=embedding
    )
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})

    # Initialize Ollama
    llm = Ollama(
        model="llama2",
        temperature=0.1
    )

    # Custom prompt template
    prompt_template = """
    ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ë‹¤ìŒ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”:

    [ë³´ì¥ í•­ëª© ë°œì·Œ]
    {context}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {question}

    ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ì œê³µí•´ì£¼ì„¸ìš”.
    ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    ë‹µë³€:
    """

    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    # Create QA chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt}
    )
    
    return qa_chain

# Initialize chatbot
if "qa_chain" not in st.session_state:
    with st.spinner("ì±—ë´‡ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘..."):
        try:
            st.session_state.qa_chain = init_chatbot()
        except Exception as e:
            st.error(f"ì±—ë´‡ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.stop()

# Sidebar for additional controls
with st.sidebar:
    st.title("ë³´í—˜ ì±—ë´‡ ì„¤ì •")
    st.markdown("---")
    
    # Model selection
    model_name = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        ["llama2", "mistral", "gemma"],
        index=0
    )
    
    # Temperature control
    temperature = st.slider(
        "ì°½ì˜ì„±",
        min_value=0.0,
        max_value=1.0,
        value=0.3,
        step=0.1,
        help="ê°’ì´ ë†’ì„ìˆ˜ë¡ ì°½ì˜ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
    )
    
    # Clear chat button
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# Main chat interface
st.title("ğŸ¤– ë³´í—˜ ì±—ë´‡")
st.caption("ë³´í—˜ ì•½ê´€ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”.")

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Generate response
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                # Get response from QA chain
                response = st.session_state.qa_chain.run(prompt)
                
                # Display response
                st.markdown(response)
                
                # Add assistant response to chat history
                st.session_state.messages.append({"role": "assistant", "content": response})
                
            except Exception as e:
                error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
