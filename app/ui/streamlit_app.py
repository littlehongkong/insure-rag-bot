import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv
import supabase

# Load environment variables
load_dotenv()

# Initialize Supabase client
supabase_url = os.getenv('SUPABASE_URL')
supabase_key = os.getenv('SUPABASE_KEY')
supabase_client = supabase.create_client(supabase_url, supabase_key)

# Page configuration
st.set_page_config(
    page_title="ë³´í—˜ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Initialize embedding model
embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

def get_embedding(text: str) -> list[float]:
    """Generate embedding for the given text"""
    return embedding_model.encode(text).tolist()

from langchain.schema import BaseRetriever, Document

class SupabaseRetriever(BaseRetriever):
    def __init__(self, supabase_client, embedding_model, k: int = 3):
        super().__init__()
        self._supabase = supabase_client
        self._embedding_model = embedding_model
        self._k = k
    
    def get_relevant_documents(self, query: str) -> list[Document]:
        """Retrieve relevant documents from Supabase using vector similarity"""
        try:
            # Generate query embedding
            query_embedding = get_embedding(query)
            
            # Query Supabase
            response = self._supabase.rpc(
                'match_documents',
                {
                    'query_embedding': query_embedding,
                    'match_count': self._k
                }
            ).execute()
            
            if not hasattr(response, 'data') or not response.data:
                return []
            
            # Convert to LangChain Document format
            documents = []
            for doc in response.data:
                documents.append(
                    Document(
                        page_content=doc.get('content', ''),
                        metadata={
                            'source': doc.get('source', ''),
                            'page': doc.get('page', 0)
                        }
                    )
                )
            return documents
            
        except Exception as e:
            st.error(f"Error retrieving documents: {str(e)}")
            return []
    
    async def aget_relevant_documents(self, query: str) -> list[Document]:
        """Async version of get_relevant_documents"""
        return self.get_relevant_documents(query)

# Initialize chatbot
def init_chatbot():
    # Initialize retriever
    retriever = SupabaseRetriever(supabase_client, embedding_model, k=3)

    # Initialize Ollama
    llm = Ollama(
        model="llama2",
        temperature=0.1
    )

    # Custom prompt template
    prompt_template = """
    ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ë‹¤ìŒ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”:

    [ë³´ì¥ í•­ëª© ë°œì·Œ]
    {context}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {question}

    ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ì œê³µí•´ì£¼ì„¸ìš”.
    ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    ë‹µë³€:
    """

    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    # Create QA chain with custom retriever
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt, "document_variable_name": "context"}
    )
    
    return qa_chain

# Initialize chatbot
if "qa_chain" not in st.session_state:
    with st.spinner("ì±—ë´‡ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘..."):
        try:
            st.session_state.qa_chain = init_chatbot()
        except Exception as e:
            st.error(f"ì±—ë´‡ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.stop()

# Sidebar for additional controls
with st.sidebar:
    st.title("ë³´í—˜ ì±—ë´‡ ì„¤ì •")
    st.markdown("---")
    
    # Model selection
    model_name = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        ["llama2", "mistral", "gemma"],
        index=0
    )
    
    # Temperature control
    temperature = st.slider(
        "ì°½ì˜ì„±",
        min_value=0.0,
        max_value=1.0,
        value=0.3,
        step=0.1,
        help="ê°’ì´ ë†’ì„ìˆ˜ë¡ ì°½ì˜ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
    )
    
    # Clear chat button
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# Main chat interface
st.title("ğŸ¤– ë³´í—˜ ì±—ë´‡")
st.caption("ë³´í—˜ ì•½ê´€ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”.")

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Generate response
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                # Get response from QA chain
                response = st.session_state.qa_chain.run(prompt)
                
                # Display response
                st.markdown(response)
                
                # Add assistant response to chat history
                st.session_state.messages.append({"role": "assistant", "content": response})
                
            except Exception as e:
                error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
