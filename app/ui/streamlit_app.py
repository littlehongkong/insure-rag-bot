import streamlit as st
from dotenv import load_dotenv
from app.services.chatbot_service import ChromaRAGChatbot

load_dotenv()

st.set_page_config(page_title="ë³´í—˜ RAG ì±—ë´‡", page_icon="ğŸ’¡")
st.title("ğŸ§  ë³´í—˜ ì±—ë´‡ (RAG ê¸°ë°˜)")

try:
    chatbot = ChromaRAGChatbot()
except ValueError as e:
    st.error(str(e))
    st.stop()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì‚¬ì´ë“œë°”ì— ì„¤ì • ì˜µì…˜ ì¶”ê°€
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    # ëª¨ë¸ ì„ íƒ
    model_option = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        ["gpt-5-nano", "gpt-5-mini"],
        index=0,
        help="gpt-5-miniëŠ” ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦½ë‹ˆë‹¤."
    )

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì„¤ì •
    max_history = st.slider(
        "ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸¸ì´",
        min_value=0,
        max_value=10,
        value=5,
        help="ì´ì „ ëŒ€í™”ë¥¼ ëª‡ ê°œê¹Œì§€ ê¸°ì–µí• ì§€ ì„¤ì •í•©ë‹ˆë‹¤. 0ì´ë©´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    )

    st.divider()

    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ’¬ ëŒ€í™” ì´ˆê¸°í™”", type="secondary"):
        st.session_state.messages = []
        st.rerun()

    # í†µê³„ ì •ë³´
    if st.session_state.messages:
        st.subheader("ğŸ“Š ëŒ€í™” í†µê³„")
        user_messages = len([m for m in st.session_state.messages if m["role"] == "user"])
        bot_messages = len([m for m in st.session_state.messages if m["role"] == "assistant"])
        st.metric("ì‚¬ìš©ì ì§ˆë¬¸", user_messages)
        st.metric("ë´‡ ë‹µë³€", bot_messages)

# ì´ì „ ëŒ€í™” ë Œë”ë§
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
if prompt := st.chat_input("ë³´í—˜ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”â€¦"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì¤‘..."):
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤€ë¹„ (max_history ê°œìˆ˜ë§Œí¼ë§Œ)
            if max_history > 0:
                # í˜„ì¬ ì§ˆë¬¸ì„ ì œì™¸í•œ ì´ì „ ëŒ€í™”ë“¤
                recent_messages = st.session_state.messages[:-1]  # í˜„ì¬ ì§ˆë¬¸ ì œì™¸
                # ìµœê·¼ ëŒ€í™”ë§Œ ì„ íƒ (user-assistant ìŒìœ¼ë¡œ)
                if len(recent_messages) > max_history * 2:
                    recent_messages = recent_messages[-(max_history * 2):]

                # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¬¸ìì—´ë¡œ êµ¬ì„±
                chat_history = []
                for i in range(0, len(recent_messages), 2):
                    if i + 1 < len(recent_messages):
                        user_msg = recent_messages[i]["content"]
                        bot_msg = recent_messages[i + 1]["content"]
                        chat_history.append(f"ì‚¬ìš©ì: {user_msg}")
                        chat_history.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {bot_msg}")

                history_context = "\n".join(chat_history) if chat_history else ""
            else:
                history_context = ""

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ì§ˆë¬¸ ìƒì„±
            if history_context and max_history > 0:
                enhanced_prompt = f"""ì´ì „ ëŒ€í™” ë‚´ìš©:
{history_context}

í˜„ì¬ ì§ˆë¬¸: {prompt}

ìœ„ì˜ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""
            else:
                enhanced_prompt = prompt

            # ëª¨ë¸ì— ë”°ë¥¸ ì‘ë‹µ ìƒì„±
            try:
                response = chatbot.ask_with_detailed_context(enhanced_prompt, model=model_option)

                # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                if not response or response["answer"].strip() == "":
                    response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."

                st.markdown(response['answer'])

            except Exception as e:
                error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_msg)
                response = error_msg

    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": response})

# í•˜ë‹¨ì— ë„ì›€ë§ í‘œì‹œ
with st.expander("ğŸ’¡ ì‚¬ìš©ë²• ë° íŒ"):
    st.markdown("""
    **ê¸°ë³¸ ì‚¬ìš©ë²•:**
    - ë³´í—˜ ê´€ë ¨ ì§ˆë¬¸ì„ ìì—°ì–´ë¡œ ì…ë ¥í•˜ì„¸ìš”
    - ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ ì—°ì†ì ì¸ ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤

    **ëª¨ë¸ ì„ íƒ:**
    - **gpt-5-nano**: ë¹ ë¥¸ ì‘ë‹µ, ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ì í•©

    **ëŒ€í™” íˆìŠ¤í† ë¦¬:**
    - ì‚¬ì´ë“œë°”ì—ì„œ ê¸°ì–µí•  ëŒ€í™” ê°œìˆ˜ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - 0ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ê° ì§ˆë¬¸ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤

    **ì˜ˆì‹œ ì§ˆë¬¸:**
    - "ì–´ë¦°ì´ ì‹¤ì†ë³´í—˜ì˜ ë³´ì¥ ë²”ìœ„ëŠ”?"
    - "ë°©ê¸ˆ ë§í•œ ë³´í—˜ì˜ ë³´í—˜ë£ŒëŠ” ì–¼ë§ˆì¸ê°€ìš”?" (ì—°ì† ì§ˆë¬¸)
    - "ë‹¤ë¥¸ ìƒí’ˆê³¼ ë¹„êµí•´ì„œ ì–´ë–¤ ì°¨ì´ì ì´ ìˆë‚˜ìš”?"
    """)

# í˜ì´ì§€ í•˜ë‹¨ì— ì •ë³´ í‘œì‹œ
st.divider()
col1, col2, col3 = st.columns(3)
with col1:
    st.caption(f"ğŸ¤– í˜„ì¬ ëª¨ë¸: {model_option}")
with col2:
    st.caption(f"ğŸ’­ ëŒ€í™” íˆìŠ¤í† ë¦¬: {max_history}ê°œ")
with col3:
    if st.session_state.messages:
        st.caption(f"ğŸ“ ì´ ë©”ì‹œì§€: {len(st.session_state.messages)}ê°œ")